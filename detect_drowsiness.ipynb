{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJc9keKkSFAELD2OmNz0iS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanishingne/Driver-drowsiness-Detection/blob/master/detect_drowsiness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Implementing Driver Drowsiness Detection algorithm using OpenCV, dlib, and Python:*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jtVQfFkFYZGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Importing the necessary packages:\n",
        "\n"
      ],
      "metadata": {
        "id": "rqcu73wwfZll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade playsound\n",
        "from scipy.spatial import distance as dist\n",
        "import imutils\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "from threading import Thread\n",
        "from playsound import playsound\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import argparse"
      ],
      "metadata": {
        "id": "679HIBw1XfZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4ddd4e-1472-4a26-fab1-9612e94189e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting playsound\n",
            "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: playsound\n",
            "  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7038 sha256=37ef92512fbe81403ae74f37c68a64a2115239f2a7df2af99ea25f45d1041c0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/cd/cf/9750b618d54bd81c20e4c34fb24a423a5b095920367cdb3f71\n",
            "Successfully built playsound\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:playsound:playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **SciPy**: We’ll need the SciPy package so we can compute the Euclidean distance between facial landmarks points in the Eye Aspect Ratio calculation.\n",
        "*   **imutils**: We’ll also use the imutils package, a series of computer vision and image processing functions to make working with OpenCV easier.\n",
        "* **Thread**: We'll import the Thread class so we can play our alarm in a separate thread from the main thread to ensure our script doesn’t pause execution while the alarm sounds.\n",
        "* **playsound**: We'll need the playsound library to play simple sounds like our MP3 alarm.\n",
        "* **dlib**: To detect and localize facial landmarks we’ll need the dlib library.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wa7gjdnyZp6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "2. Defining the sound_alarm function, which accepts a path to an audio file on the disk and then play  the sound:"
      ],
      "metadata": {
        "id": "XnhV5aHsfBRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sound_alarm(path):\n",
        "  # play an alarm sound\n",
        "  playsound.playsound(path)"
      ],
      "metadata": {
        "id": "NngNpCCKZ4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "3. Defining the `eye_aspect_ratio` function which is used to compute the ratio of distances between the vertical eye landmarks and the distances between the horizontal eye landmarks:"
      ],
      "metadata": {
        "id": "4-etGYbef7Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "  # compute the Euclidean distances between the two sets of vertical eye lankmark (x,y) coordinates:\n",
        "  A = dist.euclidean(eye[1], eye[5])\n",
        "  B = dist.euclidean(eye[2], eye[4])\n",
        "\n",
        "  # compute the Euclidean distances between the two sets of horizontal eye lankmark (x,y) coordinates:\n",
        "  C = dist.euclidean(eye[0], eye[3])\n",
        "\n",
        "  # compute the Eye-Aspect-Ratio:\n",
        "  ear = (A+B) / (2.0 * C)\n",
        "\n",
        "  # return the E.A.R:\n",
        "  return ear"
      ],
      "metadata": {
        "id": "Qn_1lEG2fy2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The EAR will be approximately constant when the eye is open, and when the eye is closed. However, the ratio will be much smaller than the ratio when the eye is open.\n",
        "* During a blink, the value will rapidly decrease towawrds zero.\n",
        "(Ref: Soukupová and Čech’s 2016 paper, [Real-Time Eye Blink Detection using Facial Landmarks](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf))\n",
        "\n",
        "* In our drowsiness detector case, we’ll be monitoring the eye aspect ratio to see if the value *falls* but does *not increase again*, thus implying that the person has closed their eyes.\n"
      ],
      "metadata": {
        "id": "0y12TedkmSfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "4. constructing the argument parser and parsing the arguments:"
      ],
      "metadata": {
        "id": "QZ8HR12hCiyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-p\", \"--shape-predictor\", required=True, help=\"path to facial landmark predictor\")\n",
        "ap.add_argument(\"-a\", \"--alarm\", type=str, default=\"\", help=\"path alarm .WAV file\")\n",
        "ap.add_argument(\"-w\", \"--webcam\", type=int, default=0, help=\"index of webcam on system\")\n",
        "args = vars(ap.parse_args())"
      ],
      "metadata": {
        "id": "J7SP3Y8xCKgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `--shape-predictor` : This is the path to dlib’s pre-trained facial landmark detector.\n",
        "* `--alarm` : Here you can optionally specify the path to an input audio file to be used as an alarm.\n",
        "* `--webcam` : This integer controls the index of your built-in webcam/USB camera."
      ],
      "metadata": {
        "id": "DjtXjDqfXFJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "5. Defining constants:"
      ],
      "metadata": {
        "id": "RRakGmLcdiXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the threshold for the EAR, indicating a \"blink\"\n",
        "EYE_AR_THRESH = 0.3\n",
        "\n",
        "# initialize the threshold for the number of consecutive frames to set off the alarm\n",
        "EYE_AR_CONSEC_FRAMES = 48\n",
        "\n",
        "# initialize the frame counter\n",
        "COUNTER = 0\n",
        "\n",
        "# initialize a boolean used to indicate if the alarm is going off\n",
        "ALARM_ON = False"
      ],
      "metadata": {
        "id": "V87g0mjTbata"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `EYE_AR_THRESH`: If the eye aspect ratio falls below this threshold, we’ll start counting the number of frames the person has closed their eyes for.\n",
        "- `EYE_AR_CONSEC_FRAMES`: If the number of frames the person has closed their eyes in exceeds EYE_AR_CONSEC_FRAMES, we’ll sound an alarm.\n",
        "- `COUNTER`: COUNTER defines the total number of consecutive frames where the eye aspect ratio is below EYE_AR_THRESH.\n",
        "- `ALARM_ON`: We'll update the boolean ALARM_ON if COUNTER exceeds EYE_AR_CONSEC_FRAMES."
      ],
      "metadata": {
        "id": "XvF3NVWAgc1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "6. Instantiate dlib's Histogram of Oriented Gradient-based face detector, along with Facial Landmark Predictor:"
      ],
      "metadata": {
        "id": "RUQOf5CPircV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] loading facial landmark predictor...\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(args[\"shape_predictor\"])"
      ],
      "metadata": {
        "id": "UmVZhrFiiU-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The facial landmark detector implemented inside dlib produces 68 (x, y)-coordinates that map to specific facial structures.\n",
        "Therefore, to extract the eye regions from a set of facial landmarks, we need to know the correct array slice indexes."
      ],
      "metadata": {
        "id": "CHF3S_eskHDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grab the indexes of the facial landmarks for the left and\n",
        "# right eye, respectively\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
      ],
      "metadata": {
        "id": "kSxKDg4vkGXd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "7. Core logic of Drowsiness detector:"
      ],
      "metadata": {
        "id": "DCwrb1GmxArS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the video stream thread\n",
        "print(\"[INFO] starting video stream thread...\")\n",
        "vs = VideoStream(src=args[\"webcam\"]).start()\n",
        "# pause to allow the camera sensor to warm up\n",
        "time.sleep(1.0)\n",
        "\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "  # grab the frame from the video stream\n",
        "  frame = vs.read()\n",
        "\t# resize it\n",
        "  frame = imutils.resize(frame, width=450)\n",
        "\t# convert it to grayscale\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # detect faces in the grayscale frame\n",
        "  rects = detector(gray, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "rB6D_hNZxL1y",
        "outputId": "549a5550-4b6b-42bc-f5ae-4a2e86c762a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] starting video stream thread...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5768d6acae80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start the video stream thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] starting video stream thread...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"webcam\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# pause to allow the camera sensor to warm up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VideoStream' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "8. Applying facial landmark detection to localize each of the important regions of the face:"
      ],
      "metadata": {
        "id": "kdBFyaUcWftB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the face detections\n",
        "for rect in rects:\n",
        "  # determine the facial landmarks for the face region\n",
        "  shape = predict(gray, rect)\n",
        "  # convert the facial landmark's (x,y) coordinates to a NumPy array\n",
        "  shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "  # extract the left and right eye coordinates\n",
        "  leftEye = shape[lStart:lEnd]\n",
        "  rightEye = shape[rStart:rEnd]\n",
        "  # compute the Eye Aspect Ratio (EAR) for both eyes using these coordinates\n",
        "  leftEAR = eye_aspect_ratio(leftEye)\n",
        "  rightEAR = eye_aspect_ratio(rightEye)\n",
        "\n",
        "  # average the EAR together for both eyes\n",
        "  ear = (leftEAR + rightEAR)/2.0"
      ],
      "metadata": {
        "id": "vCeDCXZWXdEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "baH-aGCSY0Qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}