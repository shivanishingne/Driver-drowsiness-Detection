{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOWwSalIz0d97nRCRDxV+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanishingne/Driver-drowsiness-Detection/blob/master/detect_drowsiness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Implementing Driver Drowsiness Detection algorithm using OpenCV, dlib, and Python:*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jtVQfFkFYZGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Importing the necessary packages:\n",
        "\n"
      ],
      "metadata": {
        "id": "rqcu73wwfZll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade playsound\n",
        "from scipy.spatial import distance as dist\n",
        "import imutils\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "from threading import Thread\n",
        "from playsound import playsound\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import argparse"
      ],
      "metadata": {
        "id": "679HIBw1XfZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **SciPy**: We’ll need the SciPy package so we can compute the Euclidean distance between facial landmarks points in the Eye Aspect Ratio calculation.\n",
        "*   **imutils**: We’ll also use the imutils package, a series of computer vision and image processing functions to make working with OpenCV easier.\n",
        "* **Thread**: We'll import the Thread class so we can play our alarm in a separate thread from the main thread to ensure our script doesn’t pause execution while the alarm sounds.\n",
        "* **playsound**: We'll need the playsound library to play simple sounds like our MP3 alarm.\n",
        "* **dlib**: To detect and localize facial landmarks we’ll need the dlib library.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wa7gjdnyZp6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "2. Defining the sound_alarm function, which accepts a path to an audio file on the disk and then play  the sound:"
      ],
      "metadata": {
        "id": "XnhV5aHsfBRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sound_alarm(path):\n",
        "  # play an alarm sound\n",
        "  playsound.playsound(path)"
      ],
      "metadata": {
        "id": "NngNpCCKZ4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "3. Defining the `eye_aspect_ratio` function which is used to compute the ratio of distances between the vertical eye landmarks and the distances between the horizontal eye landmarks:"
      ],
      "metadata": {
        "id": "4-etGYbef7Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "  # compute the Euclidean distances between the two sets of vertical eye lankmark (x,y) coordinates:\n",
        "  A = dist.euclidean(eye[1], eye[5])\n",
        "  B = dist.euclidean(eye[2], eye[4])\n",
        "\n",
        "  # compute the Euclidean distances between the two sets of horizontal eye lankmark (x,y) coordinates:\n",
        "  C = dist.euclidean(eye[0], eye[3])\n",
        "\n",
        "  # compute the Eye-Aspect-Ratio:\n",
        "  ear = (A+B) / (2.0 * C)\n",
        "\n",
        "  # return the E.A.R:\n",
        "  return ear"
      ],
      "metadata": {
        "id": "Qn_1lEG2fy2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The EAR will be approximately constant when the eye is open, and when the eye is closed. However, the ratio will be much smaller than the ratio when the eye is open.\n",
        "* During a blink, the value will rapidly decrease towawrds zero.\n",
        "(Ref: Soukupová and Čech’s 2016 paper, [Real-Time Eye Blink Detection using Facial Landmarks](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf))\n",
        "\n",
        "* In our drowsiness detector case, we’ll be monitoring the eye aspect ratio to see if the value *falls* but does *not increase again*, thus implying that the person has closed their eyes.\n"
      ],
      "metadata": {
        "id": "0y12TedkmSfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "4. constructing the argument parser and parsing the arguments:"
      ],
      "metadata": {
        "id": "QZ8HR12hCiyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-p\", \"--shape-predictor\", required=True, help=\"path to facial landmark predictor\")\n",
        "ap.add_argument(\"-a\", \"--alarm\", type=str, default=\"\", help=\"path alarm .WAV file\")\n",
        "ap.add_argument(\"-w\", \"--webcam\", type=int, default=0, help=\"index of webcam on system\")\n",
        "args = vars(ap.parse_args())"
      ],
      "metadata": {
        "id": "J7SP3Y8xCKgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `--shape-predictor` : This is the path to dlib’s pre-trained facial landmark detector.\n",
        "* `--alarm` : Here you can optionally specify the path to an input audio file to be used as an alarm.\n",
        "* `--webcam` : This integer controls the index of your built-in webcam/USB camera."
      ],
      "metadata": {
        "id": "DjtXjDqfXFJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "5. Defining constants:"
      ],
      "metadata": {
        "id": "RRakGmLcdiXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the threshold for the EAR, indicating a \"blink\"\n",
        "EYE_AR_THRESH = 0.3\n",
        "\n",
        "# initialize the threshold for the number of consecutive frames to set off the alarm\n",
        "EYE_AR_CONSEC_FRAMES = 48\n",
        "\n",
        "# initialize the frame counter\n",
        "COUNTER = 0\n",
        "\n",
        "# initialize a boolean used to indicate if the alarm is going off\n",
        "ALARM_ON = False"
      ],
      "metadata": {
        "id": "V87g0mjTbata"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `EYE_AR_THRESH`: If the eye aspect ratio falls below this threshold, we’ll start counting the number of frames the person has closed their eyes for.\n",
        "- `EYE_AR_CONSEC_FRAMES`: If the number of frames the person has closed their eyes in exceeds EYE_AR_CONSEC_FRAMES, we’ll sound an alarm.\n",
        "- `COUNTER`: COUNTER defines the total number of consecutive frames where the eye aspect ratio is below EYE_AR_THRESH.\n",
        "- `ALARM_ON`: We'll update the boolean ALARM_ON if COUNTER exceeds EYE_AR_CONSEC_FRAMES."
      ],
      "metadata": {
        "id": "XvF3NVWAgc1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "6. Instantiate dlib's Histogram of Oriented Gradient-based face detector, along with Facial Landmark Predictor:"
      ],
      "metadata": {
        "id": "RUQOf5CPircV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] loading facial landmark predictor...\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(args[\"shape_predictor\"])"
      ],
      "metadata": {
        "id": "UmVZhrFiiU-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "22db56f2-ecb6-4f38-ac9b-981ef2b2be42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading facial landmark predictor...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ab29d3b6dd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading facial landmark predictor...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape_predictor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The facial landmark detector implemented inside dlib produces 68 (x, y)-coordinates that map to specific facial structures.\n",
        "Therefore, to extract the eye regions from a set of facial landmarks, we need to know the correct array slice indexes."
      ],
      "metadata": {
        "id": "CHF3S_eskHDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grab the indexes of the facial landmarks for the left and\n",
        "# right eye, respectively\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
      ],
      "metadata": {
        "id": "kSxKDg4vkGXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "7. Core logic of Drowsiness detector:"
      ],
      "metadata": {
        "id": "DCwrb1GmxArS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the video stream thread\n",
        "print(\"[INFO] starting video stream thread...\")\n",
        "vs = VideoStream(src=args[\"webcam\"]).start()\n",
        "# pause to allow the camera sensor to warm up\n",
        "time.sleep(1.0)\n",
        "\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "  # grab the frame from the video stream\n",
        "  frame = vs.read()\n",
        "\t# resize it\n",
        "  frame = imutils.resize(frame, width=450)\n",
        "\t# convert it to grayscale\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # detect faces in the grayscale frame\n",
        "  rects = detector(gray, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "rB6D_hNZxL1y",
        "outputId": "ab2751a8-cc73-4ad5-d5c5-ca3a0da21fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] starting video stream thread...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5768d6acae80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start the video stream thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] starting video stream thread...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"webcam\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# pause to allow the camera sensor to warm up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "8. Applying facial landmark detection to localize each of the important regions of the face:"
      ],
      "metadata": {
        "id": "kdBFyaUcWftB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the face detections\n",
        "for rect in rects:\n",
        "  # determine the facial landmarks for the face region\n",
        "  shape = predict(gray, rect)\n",
        "  # convert the facial landmark's (x,y) coordinates to a NumPy array\n",
        "  shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "  # extract the left and right eye coordinates\n",
        "  leftEye = shape[lStart:lEnd]\n",
        "  rightEye = shape[rStart:rEnd]\n",
        "  # compute the Eye Aspect Ratio (EAR) for both eyes using these coordinates\n",
        "  leftEAR = eye_aspect_ratio(leftEye)\n",
        "  rightEAR = eye_aspect_ratio(rightEye)\n",
        "\n",
        "  # average the EAR together for both eyes\n",
        "  ear = (leftEAR + rightEAR)/2.0"
      ],
      "metadata": {
        "id": "vCeDCXZWXdEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: Soukupová and Čech recommend averaging both eye aspect ratios together to obtain a better estimation.\n",
        "\n",
        "We can then visualize each of the eye regions on our frame by using the cv2.drawContours function.\n",
        "This is helpful when we are trying to debug our script and want to ensure that the eyes are being correctly detected and localized."
      ],
      "metadata": {
        "id": "oXkTiQ-sakuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # compute the convex hull for the left and right eye\n",
        "  leftEyeHull  = cv2.convexHull(leftEye)\n",
        "  rightEyeHull = cv2.convexHull(rightEye)\n",
        "  # visualize each of the eyes\n",
        "  cv2.drawContours(frame,  [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "  cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)"
      ],
      "metadata": {
        "id": "baH-aGCSY0Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "9. Checking whether the person in our video stream is starting to show symptoms of drowsiness:"
      ],
      "metadata": {
        "id": "rM5Usp9hcJyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # check to see if the EAR is below the blink threshold:\n",
        "  if ear < EYE_AR_THRESH:\n",
        "    # increment the blink frame counter\n",
        "    COUNTER +=1    \n",
        "    # check to see if the eyes were closed for a sufficient no of frames to sound the alarm\n",
        "    if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
        "      # turn on the alarm if not no\n",
        "      if not ALARM_ON:\n",
        "        ALARM_ON = True\n",
        "        # check to see if the alarm file was supplied\n",
        "        if args[\"alarm\"] != \"\":\n",
        "          # if so, start a thread to have the alarm sound played in the bg\n",
        "          t = Thread(target=sound_alarm, args=(args[\"alarm\"],))\n",
        "          t.deamon = True\n",
        "          t.start()\n",
        "\n",
        "        # draw an alert on the frame\n",
        "        cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
        "      \n",
        "  # otherwise,the EAR is not below the blink threshold:\n",
        "    else:\n",
        "      # reset the counter and alarm\n",
        "      COUNTER = 0\n",
        "      ALARM_ON = False"
      ],
      "metadata": {
        "id": "Btj0SH-hch-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we handle the displaying of the output frame to our screen:"
      ],
      "metadata": {
        "id": "RbBvO_FFcguL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\t\t# draw the computed EAR on the frame to help with debugging\n",
        "    # and setting the correct EAR thresholds and frame counters:\n",
        "    cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        " \n",
        "\t# show the frame\n",
        "  cv2.imshow(\"Frame\", frame)\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        " \n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "  if key == ord(\"q\"):\n",
        "    break\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "metadata": {
        "id": "AHRDVgEDQZX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "10. Testing the OpenCV drowsiness detector:\n",
        "\n",
        "We can run the script, dlib's Facial Landmark Predictor, and the sample alarm audio using the following commands-\n",
        "\n",
        "\n",
        "```\n",
        "$ python detect_drowsiness.py \\\n",
        "\t--shape-predictor shape_predictor_68_face_landmarks.dat \\\n",
        "\t--alarm alarm.wav\n",
        "```\n",
        "\n",
        "\n",
        "The drowsiness detector is even able to work in a variety of conditions, including direct sunlight when driving on the road and low/artificial lighting while in the concrete parking garage."
      ],
      "metadata": {
        "id": "INrAiwS7SRHG"
      }
    }
  ]
}